{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 96\n",
    "predict_size = 96\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "n_layers = 4\n",
    "n_epoch = 3\n",
    "lr = 0.001\n",
    "\n",
    "train_split = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size : int, hidden_size : int):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_lin = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        self.input_lin = nn.Linear(input_size, 4 * hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, h_in, c_in):\n",
    "        X = self.input_lin(x) + self.hidden_lin(h_in) # 입력과 은닉 상태를 선형 변환 후 더함\n",
    "        i, f, g, o = X.chunk(4, dim=-1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "\n",
    "        c_next = c_in * f + i * g\n",
    "        h_next = torch.tanh(c_next) * o\n",
    "\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, n_layers: int):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cells = nn.ModuleList(\n",
    "            [LSTMCell(input_size=input_size, hidden_size=hidden_size)] +\n",
    "            [LSTMCell(input_size=hidden_size, hidden_size=hidden_size) for _ in range(n_layers - 1)]\n",
    "        )\n",
    "        self.linear = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        if state is None:\n",
    "            h = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n",
    "            c = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n",
    "        else:\n",
    "            h, c = state\n",
    "            h, c = list(torch.unbind(h)), list(torch.unbind(c))\n",
    "\n",
    "        outputs = []  # 각 time step의 출력을 담는 리스트\n",
    "        for t in range(seq_len):\n",
    "            inp = x[:, t, :]  # 각 시점의 입력\n",
    "            for layer in range(self.n_layers):\n",
    "                h[layer], c[layer] = self.cells[layer](inp, h[layer], c[layer])\n",
    "                inp = h[layer]\n",
    "            outputs.append(self.linear(h[-1]))  # 각 time step에서 마지막 layer의 hidden state를 사용해 예측\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=0)  # 모든 time step의 예측을 쌓음\n",
    "        h = torch.stack(h)\n",
    "        c = torch.stack(c)\n",
    "        return outputs[:, -predict_size:, -1], (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv(\"../dataset/gc19_a.csv\")\n",
    "uni_data = data_csv['avgcpu']\n",
    "uni_data.index = data_csv['time'] # index로 매핑\n",
    "\n",
    "uni_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size, step, single_step=False):\n",
    "    datas = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "\n",
    "    if end_index == None:\n",
    "        end_index = len(dataset) - target_size\n",
    "    \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        datas.append(np.reshape(dataset[indices], (history_size,1)))\n",
    "\n",
    "        if single_step: # 단기 예측\n",
    "            labels.append(dataset[i + target_size])\n",
    "        else: # 장기 예측\n",
    "            labels.append(dataset[i:i + target_size])\n",
    "\n",
    "    return np.array(datas), np.array(labels)\n",
    "\n",
    "def multivariate_data(dataset, start_index, end_index, history_size, target_size, step, single_step=False):\n",
    "    datas = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "\n",
    "    if end_index == None:\n",
    "        end_index = len(dataset) - target_size\n",
    "    \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        datas.append(dataset[indices])\n",
    "\n",
    "        if single_step: # 단기 예측\n",
    "            labels.append(dataset[i + target_size])\n",
    "        else: # 장기 예측\n",
    "            labels.append(dataset[i:i + target_size])\n",
    "\n",
    "    return np.array(datas), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "uni_data = uni_data.values\n",
    "uni_train_mean = uni_data[:train_split].mean()\n",
    "uni_train_std = uni_data[:train_split].std()\n",
    "\n",
    "uni_data = (uni_data - uni_train_mean) / uni_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, train_split, history_size, predict_size, 1, False)\n",
    "x_test_uni, y_test_uni = univariate_data(uni_data, train_split, None, history_size, predict_size, 1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "    if length >= 0:\n",
    "        return range(0, length)\n",
    "    else:\n",
    "        return range(length, 0)\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"history\", \"true future\", \"baseline\"]\n",
    "    marker = [\"-\", \"r-\", \"g-\"]\n",
    "    time_steps = create_time_steps(-plot_data[0].shape[0])\n",
    "    if delta: future = delta\n",
    "    else: future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(create_time_steps(x.shape[0]), plot_data[i], marker[i], label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.axis('auto')\n",
    "    plt.xlabel('time-steps')\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[0], y_train_uni[0]], predict_size, \"predict by avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, n_layers=n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_uni = DataLoader(dataset=torch.tensor(x_train_uni, dtype=torch.float), batch_size=100, num_workers=2, shuffle=False)\n",
    "# y_train_uni = DataLoader(dataset=torch.tensor(y_train_uni, dtype=torch.float), batch_size=100, num_workers=2, shuffle=False)\n",
    "\n",
    "x_train_uni = torch.tensor(x_train_uni, dtype=torch.float)\n",
    "y_train_uni = torch.tensor(y_train_uni, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        h, c = None, None  # 초기화\n",
    "        for batch, data in enumerate(X):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.unsqueeze(0)\n",
    "            # 이전 배치에서 hidden state와 cell state가 있으면 detach()\n",
    "            if h is not None and c is not None:\n",
    "                h = h.detach()\n",
    "                c = c.detach()\n",
    "                pred, (h, c) = model(data, (h,c))  # 상태를 전달\n",
    "            else:\n",
    "            # 모델의 출력과 새로운 hidden state, cell state\n",
    "                pred, (h, c) = model(data, None)  # 상태를 전달\n",
    "\n",
    "            # 손실 계산 및 역전파\n",
    "            loss = criterion(Y[batch], pred.squeeze(1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # 100번째 배치마다 손실 출력\n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Epoch: {ep}; Batch: {batch}; Loss: {loss.item()};\")\n",
    "            if batch % 1000 == 0:\n",
    "                show_plot([data[0, :], Y[batch], pred.detach().numpy()], predict_size, \"predict by lstm\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(x_train_uni, y_train_uni, model, criterion, optimizer, n_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
