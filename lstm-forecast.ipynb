{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 96\n",
    "predict_size = 96\n",
    "hidden_size = 256\n",
    "batch = 50\n",
    "n_layers = 16\n",
    "n_epoch = 100\n",
    "lr = 0.001\n",
    "\n",
    "train_split = predict_size * batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size : int, hidden_size : int):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_lin = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        self.input_lin = nn.Linear(input_size, 4 * hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, h_in, c_in):\n",
    "        X = self.input_lin(x) + self.hidden_lin(h_in) # 입력과 은닉 상태를 선형 변환 후 더함\n",
    "        i, f, g, o = X.chunk(4, dim=-1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "\n",
    "        c_next = c_in * f + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, n_layers: int, use_RevIN: bool):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_Revin = use_RevIN\n",
    "        self.cells = nn.ModuleList(\n",
    "            [LSTMCell(input_size=input_size, hidden_size=hidden_size)] +\n",
    "            [LSTMCell(input_size=hidden_size, hidden_size=hidden_size) for _ in range(n_layers - 1)]\n",
    "        )\n",
    "        self.linear = nn.Linear(self.hidden_size, 1)\n",
    "        self.revin = RevIN(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        if self.use_Revin:\n",
    "            x = self.revin(x, \"norm\")\n",
    "\n",
    "        if state is None:\n",
    "            h = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n",
    "            c = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n",
    "        else:\n",
    "            h, c = state\n",
    "            h, c = list(torch.unbind(h)), list(torch.unbind(c))\n",
    "\n",
    "        outputs = []  # 각 time step의 출력을 담는 리스트\n",
    "        for t in range(seq_len):\n",
    "            inp = x[t, :].squeeze(-1)  # 각 시점의 입력\n",
    "            for layer in range(self.n_layers):\n",
    "                h[layer], c[layer] = self.cells[layer](inp, h[layer], c[layer])\n",
    "                inp = h[layer]\n",
    "            outputs.append(self.linear(h[-1]))  # 각 time step에서 마지막 layer의 hidden state를 사용해 예측\n",
    "        outputs = torch.stack(outputs, dim=1).squeeze(0)  # 모든 time step의 예측을 쌓음\n",
    "\n",
    "        if self.use_Revin:\n",
    "            outputs = self.revin(outputs, \"denorm\").squeeze(0)\n",
    "\n",
    "        h = torch.stack(h)\n",
    "        c = torch.stack(c)\n",
    "\n",
    "        return outputs[-predict_size:], (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv(\"./cpu_mem/gc19_a.csv\")\n",
    "uni_data = data_csv['avgcpu']\n",
    "#uni_data.index = data_csv['time'] # index로 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size, step, single_step=False):\n",
    "    datas = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "\n",
    "    if end_index == None:\n",
    "        end_index = len(dataset) - target_size\n",
    "    \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        datas.append(np.reshape(dataset[indices], (history_size,1)))\n",
    "\n",
    "        if single_step: # 단기 예측\n",
    "            labels.append(dataset[i + target_size])\n",
    "        else: # 장기 예측\n",
    "            labels.append(dataset[i:i + target_size])\n",
    "\n",
    "    return np.array(datas), np.array(labels)\n",
    "\n",
    "def multivariate_data(dataset, start_index, end_index, history_size, target_size, step, single_step=False):\n",
    "    datas = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "\n",
    "    if end_index == None:\n",
    "        end_index = len(dataset) - target_size\n",
    "    \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        datas.append(dataset[indices])\n",
    "\n",
    "        if single_step: # 단기 예측\n",
    "            labels.append(dataset[i + target_size])\n",
    "        else: # 장기 예측\n",
    "            labels.append(dataset[i:i + target_size])\n",
    "\n",
    "    return np.array(datas), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, train_split, history_size, predict_size, 1, False)\n",
    "x_test_uni, y_test_uni = univariate_data(uni_data, train_split, None, history_size, predict_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "    if length >= 0:\n",
    "        return range(0, length)\n",
    "    else:\n",
    "        return range(length, 0)\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"history\", \"true future\", \"baseline\", \"mean\"]\n",
    "    marker = [\"-\", \"r-\", \"g-\", \"yx\"]\n",
    "    time_steps = create_time_steps(-plot_data[0].shape[0])\n",
    "    if delta: future = delta\n",
    "    else: future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i == 3:\n",
    "            plt.plot(1, plot_data[i], marker[i], label=labels[i])\n",
    "        elif i:\n",
    "            plt.plot(create_time_steps(x.shape[0]), plot_data[i], marker[i], label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.axis('auto')\n",
    "    plt.xlabel('time-steps')\n",
    "    return plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def forward(self,y,y_hat):\n",
    "        return torch.sqrt(self.mse(y,y_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=predict_size, hidden_size=hidden_size, n_layers=n_layers, use_RevIN=True).to(device)\n",
    "criterion = RMSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_uni = DataLoader(dataset=torch.tensor(x_train_uni, dtype=torch.float), batch_size=100, num_workers=2, shuffle=False)\n",
    "# y_train_uni = DataLoader(dataset=torch.tensor(y_train_uni, dtype=torch.float), batch_size=100, num_workers=2, shuffle=False)\n",
    "\n",
    "x_train_uni = torch.tensor(x_train_uni, dtype=torch.float)\n",
    "y_train_uni = torch.tensor(y_train_uni, dtype=torch.float)\n",
    "\n",
    "x_train_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        h, c = None, None  # 초기화\n",
    "        for batch in range(predict_size, len(X), predict_size):\n",
    "            optimizer.zero_grad()\n",
    "            data = X[batch - predict_size: batch]\n",
    "            # 이전 배치에서 hidden state와 cell state가 있으면 detach()\n",
    "            \n",
    "            pred = model(data)\n",
    "            # 손실 계산 및 역전파\n",
    "            loss = criterion(Y[batch - predict_size], pred[0])\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # 100번째 배치마다 손실 출력\n",
    "            if batch % predict_size == 0:\n",
    "                print(f\"Epoch: {ep}; Batch: {batch}; Loss: {loss.item()};\")\n",
    "            if batch % (5 * predict_size) == 0:\n",
    "                show_plot([data[0, :].cpu(), Y[batch - predict_size].cpu(), pred[0][:,-1].cpu().detach().squeeze(-1).numpy(), data[0, :].cpu().mean()], predict_size, \"predict by lstm\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(x_train_uni.to(device), y_train_uni.to(device), model, criterion, optimizer, n_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
